# Walkthrough: Deploying a Hugging Face Model as a Worker Node on the Allora Network

> This guide provides a step-by-step process to deploy a Hugging Face model as a Worker Node within the Allora Network. By following these instructions, you will be able to integrate and run models from Hugging Face, contributing to the Allora decentralized machine intelligence ecosystem.


## Prerequisites

Before you start, ensure you have the following:

- A Python environment with `pip` installed.
- A Docker environment with `docker compose` installed.
- Basic knowledge of machine learning and the [Hugging Face](https://huggingface.co/) ecosystem.
- Familiarity with Allora Network documentation on [allocmd](./deploy-worker-with-allocmd) and [building and deploying a worker node from scratch](./build-and-deploy-worker-from-scratch).


## Overview

During this walkthrough, we will build a worker node from an existing Hugging Face model to deploy and participate on the Allora Network. We will use this model to predict the price of BTC in 24h. 

In this example, we will use the Chronos model: [amazon/chronos-t5-tiny](https://huggingface.co/amazon/chronos-t5-tiny). 
Chronos is a family of pretrained time series forecasting models based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.
For simplicity, we will use Zero-shot forecasting, which refers to the ability of models to generate forecasts from unseen datasets.

Our worker will provide inferences on the BTC 24h Prediction, which is Topic `3` on Allora.

> Note:
> To deploy on the Allora Network, you will need to [pick the topic ID](../devs/existing-topics) you wish to generate inference for, or [create a new topic](../devs/how-to-create-topic).

We will use [Coingecko](https://www.coingecko.com/en/api) to fetch the data. You will need to [create an API key](https://www.coingecko.com/en/api/pricing).


## Installing allocmd

First, install `allocmd` as [explained in the documentation](./deploy-worker-with-allocmd):

```bash
pip install allocmd --upgrade
```
At the time of writing, the latest `allocmd` version is `2.0.3`:
```bash
allocmd --version
allocmd version 2.0.3
```

## Initializing the worker for development

Initialize the worker with your preferred name and topic ID in a development environment:

```bash
allocmd generate worker --name <workerName> --topic 3 --env dev
cd <workerName>/worker
```

```
tree .
.
├── Dockerfile
├── config.yaml
├── data
│   ├── head
│   │   └── key
│   │       ├── identity
│   │       ├── priv.bin
│   │       ├── priv.txt
│   │       ├── pub.bin
│   │       └── pubkey.txt
│   └── worker
│       └── key
│           ├── identity
│           ├── priv.bin
│           ├── priv.txt
│           ├── pub.bin
│           └── pubkey.txt
├── dev-docker-compose.yaml
├── main.py
└── requirements.txt

6 directories, 15 files
```


## Creating the inference server

We will create a very simple Flask application to serve inference from the Hugging Face model.

Here is an example of our newly created `app.py`:

```python
from flask import Flask, Response
import requests
import json
import pandas as pd
import torch
from chronos import ChronosPipeline

# create our Flask app
app = Flask(__name__)

# define the Hugging Face model we will use
model_name = "amazon/chronos-t5-tiny"

# define our endpoint
@app.route("/inference/<string:token>")
def get_inference(token):
    """Generate inference for given token."""
    if not token or token != "BTC":
        error_msg = "Token is required" if not token else "Token not supported"
        return Response(json.dumps({"error": error_msg}), status=400, mimetype='application/json')
    try:
        # use a pipeline as a high-level helper
        pipeline = ChronosPipeline.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
        )
    except Exception as e:
        return Response(json.dumps({"pipeline error": str(e)}), status=500, mimetype='application/json')

    # get the data from Coingecko
    # here we'll use last 30 days of BTC/USD
    url = "https://api.coingecko.com/api/v3/coins/bitcoin/market_chart?vs_currency=usd&days=30&interval=daily"

    headers = {
        "accept": "application/json",
        "x-cg-demo-api-key": "CG-API_KEY" # replace with your API key
    }

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        df = pd.DataFrame(data["prices"])
        df.columns = ["date", "price"]
        df["date"] = pd.to_datetime(df["date"], unit = "ms")
        df = df[:-1] # removing today's price
        print(df.tail(5))
    else:
        return Response(json.dumps({"Failed to retrieve data from the API": str(response.text)}), 
                        status=response.status_code, 
                        mimetype='application/json')

    # define the context and the prediction length
    context = torch.tensor(df["price"])
    prediction_length = 1

    try:
        forecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]
        print(forecast[0].mean().item()) # taking the mean of the forecasted prediction
        return Response(str(forecast[0].mean().item()), status=200)
    except Exception as e:
        return Response(json.dumps({"error": str(e)}), status=500, mimetype='application/json')

# run our Flask app
if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)
```

## Modifying requirements.txt

Update the `requirements.txt` to include the necessary packages for the inference server:

```
flask[async]
gunicorn[gthread]
transformers[torch]
pandas
git+https://github.com/amazon-science/chronos-forecasting.git
```

## Modifying main.py to call the inference server

Update `main.py` to integrate your worker node with the inference server:

```python
import requests
import sys
import json

def process(argument):
    headers = {'Content-Type': 'application/json'}
    url = f"http://inference:8000/inference"
    response = requests.get(url, headers=headers)
    return response.text

if __name__ == "__main__":
    # Your code logic with the parsed argument goes here
    try:
        if len(sys.argv) < 5:
            value = json.dumps({"error": f"Not enough arguments provided: {len(sys.argv)}, expected 4 arguments: topic_id, blockHeight, blockHeightEval, default_arg"})
        else:
            topic_id = sys.argv[1]
            blockHeight = sys.argv[2]
            blockHeightEval = sys.argv[3]
            default_arg = sys.argv[4]
            
            response_inference = process(argument=default_arg)
            response_dict = {"infererValue": response_inference}
            value = json.dumps(response_dict)
    except Exception as e:
        value = json.dumps({"error": {str(e)}})
    print(value)
```

## Updating the Docker configuration

Modify the generated `Dockerfile` for the head and worker nodes:

```dockerfile
FROM alloranetwork/allora-inference-base:latest

RUN pip install requests

COPY main.py /app/
```

And create the `Dockerfile_inference` for the inference server:

```dockerfile
FROM amd64/python:3.9-buster

WORKDIR /app

COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --upgrade pip \
    && pip install -r requirements.txt

EXPOSE 8000

ENV NAME sample

# Run gunicorn when the container launches and bind port 8000 from app.py
CMD ["gunicorn", "-b", ":8000", "app:app"]
```

Finally, add the inference service in the `dev-docker-compose.yaml`:

```dockerfile
[...]
services:
  inference:
    container_name: inference-hf
    build:
      context: .
      dockerfile: Dockerfile_inference
    command: python -u /app/app.py
    ports:
      - "8000:8000"
    networks:
      b7s-local:
        aliases:
          - inference
        ipv4_address: 172.19.0.4
[...]
```

## Testing our worker node

Now that everything is set up correctly, we can build our containers with the following command:

```bash
docker compose -f dev-docker-compose.yaml up --build
```

After a few minutes, you will see your Flask application running in the logs:
```bash
inference-hf  |  * Serving Flask app 'app'
```

To test our inference server first by directly querying it. To do that, we can issue the following HTTP request:

```bash
curl http://127.0.0.1:8000/inference/BTC
```

And we have a response!
```bash
66600.6124078708
```

Now that we know our inference server is working as expected, lets ensure it can interact with the [Blockless network](https://blockless.network/). This is how Allora nodes respond to [requests for inference from chain validators](../learn/architecture.mdx#inferences).

We can issue a Blockless request with this command, by specifying the correct `<topicId>`:

```bash
curl --location 'http://localhost:6000/api/v1/functions/execute' \
--header 'Content-Type: application/json' \
--data '{
    "function_id": "bafybeigpiwl3o73zvvl6dxdqu7zqcub5mhg65jiky2xqb4rdhfmikswzqm",
    "method": "allora-inference-function.wasm",
    "parameters": null,
    "topic": "3",
    "config": {
        "env_vars": [
            {
                "name": "BLS_REQUEST_PATH",
                "value": "/api"
            },
            {
                "name": "ALLORA_ARG_PARAMS",
                "value": "BTC"
            }
        ],
        "number_of_nodes": -1,
        "timeout": 2
    }
}' | jq
```

And here is the response:

```json
{
  "code": "200",
  "request_id": "779d2b33-52ad-4d3e-8981-24b65045dd0b",
  "results": [
    {
      "result": {
        "stdout": "{\"infererValue\": \"66600.6124078708\"}\n\n",
        "stderr": "",
        "exit_code": 0
      },
      "peers": [
        "12D3KooWSeEuvfhDKPsjpJYhV5ZnCufdHbh5eSVqSZbToQ3aFaju"
      ],
      "frequency": 100
    }
  ],
  "cluster": {
    "peers": [
      "12D3KooWSeEuvfhDKPsjpJYhV5ZnCufdHbh5eSVqSZbToQ3aFaju"
    ]
  }
}
```

Congratulations! Your worker node running the Hugging Face model is now up and running locally on your machine. We've also verified that it can participate in Allora by responding to Blockless requests.


## Initializing the worker for production

Your worker node is now ready to be deployed!
The following command will handle the generation of the `prod-docker-compose.yaml` file which contains all the keys and parameters needed for your worker to function perfectly in production:

```bash
allocmd generate worker --env prod
chmod -R +rx ./data/scripts
```

By running this command, `prod-docker-compose.yaml` will be generated with appropriate keys and parameters. 
> You will need to modify this file to add your inference service, as you did for `dev-docker-compose.yaml`.

You can now run the `prod-docker-compose.yaml` file with: 
```bash
docker compose -f prod-docker-compose.yaml up
```
or deploy the whole codebase in your preferred cloud instance. 

At this stage, your worker should be responding to inference request from the Allora Chain - Congratulations!


## Next steps

This basic example illustrates how to integrate a Hugging Face model in an Allora worker node. 
There are now several ways for you to improve this code:
- Use your own data or another data provider, or add more features to improve the accuracy 
- Use a different model size from Chronos to compare the results
- Change the accepted token or the timeframe to predict on another [Allora topic](../devs/existing-topics)
- Use another model from Hugging Face or your own model to deploy a worker node on the Allora Network!